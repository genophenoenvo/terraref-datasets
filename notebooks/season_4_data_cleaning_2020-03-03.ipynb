{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAC Season 4 Data Cleaning\n",
    "#### Traits\n",
    "- aboveground dry biomass\n",
    "- days & growing degree days (GDD) to flowering\n",
    "- days & GDD to flag leaf emergence\n",
    "- canopy height (time series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code Emily Cain used to clean and curate sorghum data from MAC Season 4. The input csv files were queried from betydb version 1 in February 2020 and from the MAC weather station website. To run the entire notebook (in the CyVerse Discovery Environment, for now, where the input data are stored) and output the csv files for the traits above, select `Run` and then `Run All Cells` from the notebook menu above. Once all cells have been executed, the output csv file will appear in the file panel on the left. Right click to download the file. If you experience any problems or have questions, please e-mail ejcain@arizona.edu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_subplots(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe as argument and checks for sitename subplots ending in ' E' or ' W'\n",
    "    Will return rows with subplots, if any.\n",
    "    \"\"\"\n",
    "    return df.loc[(df.sitename.str.endswith(' E')) | (df.sitename.str.endswith(' W'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Read in dataset\n",
    "Future: query betydb directly with public API key for most recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv('mac_season_four_2020-02-26.csv', low_memory=False)\n",
    "print(df_0.shape)\n",
    "df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Unnamed: 0', 'checked', 'result_type', 'id', 'citation_id', 'site_id', 'treatment_id', 'city', \n",
    "                'scientificname', 'commonname', 'genus', 'species_id', 'cultivar_id', 'author',\n",
    "                'citation_year', 'time', 'raw_date', 'month', 'year', 'dateloc', 'n', 'statname', 'stat', 'notes', \n",
    "                'access_level', 'entity', 'view_url', 'edit_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_0.drop(labels=cols_to_drop, axis=1)\n",
    "print(df_1.shape)\n",
    "df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df_1.columns:\n",
    "#     print(f'{col}: {df_1[col].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Change `date` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dates = []\n",
    "\n",
    "for d in df_1.date.values:\n",
    "    \n",
    "    # strip '(America/Phoenix)' string from date\n",
    "    if 'Phoenix' in d:\n",
    "        new_name = d[:-18]\n",
    "        new_dates.append(new_name)\n",
    "    \n",
    "    else:\n",
    "        new_name = d\n",
    "        new_dates.append(new_name)\n",
    "        \n",
    "\n",
    "# check that length of new dates matches number of rows\n",
    "print(len(new_dates))\n",
    "print(df_1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string dates to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_format_dates = pd.to_datetime(new_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new column with datetime values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df to avoid SettingWithCopyWarning\n",
    "df_2 = df_1.copy()\n",
    "df_2['date_1'] = iso_format_dates\n",
    "\n",
    "print(df_2.shape)\n",
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Extract Range & Column Values for Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.copy()\n",
    "\n",
    "df_3['range'] = df_3['sitename'].str.extract(\"Range (\\d+)\").astype(int)\n",
    "df_3['column'] = df_3['sitename'].str.extract(\"Column (\\d+)\").astype(int)\n",
    "\n",
    "df_3.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI. Drop, Rename, & Reorder Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop string date column\n",
    "\n",
    "df_4 = df_3.drop(labels=['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_4.rename({'date_1': 'date', 'mean': 'value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['sitename', 'range', 'column', 'lat', 'lon', 'date', 'treatment', 'trait', 'trait_description', 'method_name', 'cultivar', 'value', 'units']\n",
    "\n",
    "df_6 = pd.DataFrame(data=df_5, columns=new_col_order).reset_index(drop=True)\n",
    "print(df_6.shape)\n",
    "df_6.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI. Select for specific traits\n",
    "- aboveground dry biomass\n",
    "- days & GDD to flowering\n",
    "- days & GDD to flag leaf emergence\n",
    "- canopy height - time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Aboveground Dry Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_df = df_6.loc[df_6.trait == 'aboveground_dry_biomass']\n",
    "print(adb_df.shape)\n",
    "adb_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for E and W subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will have no output if there are no subplots\n",
    "\n",
    "check_for_subplots(adb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write dataframe to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'aboveground_dry_biomass_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "adb_df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Days & Growing Degree Days (GDD) to Flowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5.trait.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_0 = df_6.loc[df_6.trait == 'flowering_time']\n",
    "print(flower_df_0.shape)\n",
    "flower_df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for E and W subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will have no output if there are no subplots\n",
    "\n",
    "check_for_subplots(flower_df_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Season Four Weather Data from MAC Weather Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_0 = pd.read_csv('mac_weather_station_raw_daily_2017.csv')\n",
    "print(weather_df_0.shape)\n",
    "weather_df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice dataframe for season dates only and add date column\n",
    "* Planting Date: 2017-04-20, Day 110\n",
    "* Last Day of Harvest: 2017-09-16, Day 259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_1 = weather_df_0.loc[(weather_df_0.day_of_year >= 110) & (weather_df_0.day_of_year <= 259)]\n",
    "print(weather_df_1.shape)\n",
    "weather_df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_date_range = pd.date_range(start='2017-04-20', end='2017-09-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_2 = weather_df_1.copy()\n",
    "weather_df_2['date'] = season_4_date_range\n",
    "print(weather_df_2.shape)\n",
    "weather_df_2.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Growing Degree Days\n",
    "- Future: add LaTeX equation\n",
    "- Future: add info about min and max daily values\n",
    "- 10 degrees Celsius is base temp for sorghum\n",
    "- Daily gdd value = ((max temp + min temp) / 2) - 10 (base temp)\n",
    "- Growing Degree Days = cumulative sum of daily gdd values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_3 = weather_df_2.copy()\n",
    "weather_df_3['daily_gdd'] = (((weather_df_3['air_temp_max'] + weather_df_3['air_temp_min'])) / 2) - 10\n",
    "print(weather_df_3.shape)\n",
    "weather_df_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_4 = weather_df_3.copy()\n",
    "weather_df_4['gdd'] = np.rint(np.cumsum(weather_df_4['daily_gdd']))\n",
    "print(weather_df_4.shape)\n",
    "weather_df_4.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add planting date 2017-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_planting = datetime.date(2017,4,20)\n",
    "flower_df_1 = flower_df_0.copy()\n",
    "\n",
    "flower_df_1['date_of_planting'] = day_of_planting\n",
    "print(flower_df_1.shape)\n",
    "flower_df_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create timedelta using days to flowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_values = flower_df_1['value'].values\n",
    "dates_of_flowering = []\n",
    "\n",
    "for val in timedelta_values:\n",
    "    \n",
    "    date_of_flowering = day_of_planting + datetime.timedelta(days=val)\n",
    "    dates_of_flowering.append(date_of_flowering)\n",
    "    \n",
    "print(flower_df_1.shape[0])\n",
    "print(len(dates_of_flowering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_2 = flower_df_1.copy()\n",
    "flower_df_2['date_of_flowering'] = dates_of_flowering\n",
    "print(flower_df_2.shape)\n",
    "flower_df_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add GDD to flowering dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df for date and cumulative gdd values only\n",
    "\n",
    "season_4_gdd = weather_df_4[['date', 'gdd']]\n",
    "print(season_4_gdd.shape)\n",
    "season_4_gdd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_3 = flower_df_2.copy()\n",
    "flower_df_3.date_of_flowering = pd.to_datetime(flower_df_3.date_of_flowering)\n",
    "flower_df_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_4 = flower_df_3.merge(season_4_gdd, how='left', left_on='date_of_flowering', right_on='date')\n",
    "print(flower_df_4.shape)\n",
    "flower_df_4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all date columns except `date_of_flowering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols_to_drop = ['date_x', 'date_of_planting', 'date_y']\n",
    "flower_df_5 = flower_df_4.drop(labels=date_cols_to_drop, axis=1)\n",
    "print(flower_df_5.shape)\n",
    "flower_df_5.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_5.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write dataframe to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'days_gdd_to_flowering_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "flower_df_5.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Days & GDD to Flag Leaf Emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_0 = df_6.loc[df_6.trait == 'flag_leaf_emergence_time']\n",
    "print(fle_0.shape)\n",
    "fle_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for E and W subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will have no output if there are no subplots\n",
    "\n",
    "check_for_subplots(fle_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Season Four Weather Data from MAC Weather Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df_0 = pd.read_csv('mac_weather_station_raw_daily_2017.csv')\n",
    "print(weather_df_0.shape)\n",
    "weather_df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice dataframe for season dates only and add date column\n",
    "* Planting Date: 2017-04-20, Day 110\n",
    "* Last Day of Harvest: 2017-09-16, Day 259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_1 = weather_df_0.loc[(weather_df_0.day_of_year >= 110) & (weather_df_0.day_of_year <= 259)]\n",
    "print(weather_df_1.shape)\n",
    "weather_df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_date_range = pd.date_range(start='2017-04-20', end='2017-09-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_2 = weather_df_1.copy()\n",
    "weather_df_2['date'] = season_4_date_range\n",
    "print(weather_df_2.shape)\n",
    "weather_df_2.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Growing Degree Days\n",
    "- Future: add LaTeX equation\n",
    "- Future: add info about min and max daily values\n",
    "- 10 degrees Celsius is base temp for sorghum\n",
    "- Daily gdd value = ((max temp + min temp) / 2) - 10 (base temp)\n",
    "- Growing Degree Days = cumulative sum of daily gdd values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_3 = weather_df_2.copy()\n",
    "weather_df_3['daily_gdd'] = (((weather_df_3['air_temp_max'] + weather_df_3['air_temp_min'])) / 2) - 10\n",
    "print(weather_df_3.shape)\n",
    "weather_df_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_4 = weather_df_3.copy()\n",
    "\n",
    "# round to the nearest integer\n",
    "weather_df_4['gdd'] = np.rint(np.cumsum(weather_df_4['daily_gdd']))\n",
    "print(weather_df_4.shape)\n",
    "weather_df_4.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add planting date 2017-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_planting = datetime.date(2017,4,20)\n",
    "fle_1 = fle_0.copy()\n",
    "\n",
    "fle_1['date_of_planting'] = day_of_planting\n",
    "print(fle_1.shape)\n",
    "fle_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create timedelta using days to flag leaf emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_values = fle_1['value'].values\n",
    "dates_of_flag_leaf_emergence = []\n",
    "\n",
    "for val in timedelta_values:\n",
    "    \n",
    "    date_of_flag_leaf_emergence = day_of_planting + datetime.timedelta(days=val)\n",
    "    dates_of_flag_leaf_emergence.append(date_of_flag_leaf_emergence)\n",
    "    \n",
    "print(fle_1.shape[0])\n",
    "print(len(dates_of_flag_leaf_emergence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_2 = fle_1.copy()\n",
    "fle_2['date_of_flag_leaf_emergence'] = dates_of_flag_leaf_emergence\n",
    "print(fle_2.shape)\n",
    "fle_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add GDD to flag leaf emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df for date and cumulative gdd values only\n",
    "\n",
    "season_4_gdd = weather_df_4[['date', 'gdd']]\n",
    "print(season_4_gdd.shape)\n",
    "season_4_gdd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_3 = fle_2.copy()\n",
    "fle_3.date_of_flag_leaf_emergence = pd.to_datetime(fle_3.date_of_flag_leaf_emergence)\n",
    "fle_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_4 = fle_3.merge(season_4_gdd, how='left', left_on='date_of_flag_leaf_emergence', right_on='date')\n",
    "print(fle_4.shape)\n",
    "fle_4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all date columns except `date_of_flag_leaf_emergence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols_to_drop = ['date_x', 'date_of_planting', 'date_y']\n",
    "fle_5 = fle_4.drop(labels=date_cols_to_drop, axis=1)\n",
    "print(fle_5.shape)\n",
    "fle_5.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_5.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep duplicates for now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write dataframe to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'days_gdd_to_flag_leaf_emergence_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "fle_5.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Canopy Height - Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_0 = df_6.loc[df_6.trait == 'canopy_height']\n",
    "print(ch_0.shape)\n",
    "ch_0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots = check_for_subplots(ch_0)\n",
    "subplots.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take average canopy height values for subplots on same day\n",
    "- Strip ` E` and ` W` subplot designations\n",
    "- Group by rows with the same sitename and date and take the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitename_values = ch_0.sitename.values\n",
    "no_e_w_names = []\n",
    "\n",
    "for name in sitename_values:\n",
    "    \n",
    "    if name.endswith(' W') | name.endswith(' E'):\n",
    "        name = name[:-2]\n",
    "        no_e_w_names.append(name)\n",
    "        \n",
    "    else:\n",
    "        no_e_w_names.append(name)\n",
    "        \n",
    "print(len(no_e_w_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new sitename column with no subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_1 = ch_0.copy()\n",
    "ch_1['sitename_1'] = no_e_w_names\n",
    "print(ch_1.shape)\n",
    "ch_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sqlite database to group by `sitename_1` and `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('canopy_heights.sqlite')\n",
    "cursor = conn.cursor()\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment next line out if db has already been created\n",
    "ch_1.to_sql('canopy_heights.sqlite', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_2 = pd.read_sql_query(\"\"\"\n",
    "                            SELECT sitename_1 AS sitename, range, column, lat, lon, date, treatment, \n",
    "                            trait, trait_description, method_name, cultivar, \n",
    "                            ROUND(AVG(value), 2) AS avg_canopy_height, units \n",
    "                            FROM 'canopy_heights.sqlite'\n",
    "                            GROUP BY sitename_1, date, cultivar\n",
    "                            ORDER BY date ASC;\n",
    "                            \"\"\", conn)\n",
    "\n",
    "print(ch_2.shape)\n",
    "ch_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "\n",
    "sample_with_subplot = ch_1.loc[(ch_1.range == 5) & (ch_1.column == 7) & (ch_1.date == '2017-07-11')]\n",
    "sample_with_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check - should have only one row for the above group\n",
    "\n",
    "sample_without_subplot = ch_2.loc[(ch_2.range == 5) & (ch_2.column == 7) & (ch_2.date == '2017-07-11 00:00:00')]\n",
    "sample_without_subplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write dataframe to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'canopy_height_time_series_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "ch_2.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
